# 043 – Validation and Debug Reasoning for LLM-Suggested Prompt Commands

- Status: Accepted  
- Date: 2025-12-11  
- Context: `talon-ai-tools` GPT `model suggest` flow, prompt recipe suggestions GUI, and ADRs 008/040/042.

---

## Summary (for users)

This ADR adds two guardrails to the `model suggest` feature:

1. **Command validation** – Every *LLM-suggested* command string (especially stance commands) is validated against the actual Talon grammar and axis vocab before being shown in the suggestions GUI. Invalid commands are suppressed from the UI so users never see or speak them.

2. **Debug reasoning capture** – For each suggestion, the LLM also returns a short "reasoning" string describing *how it chose that suggestion* (for example, why that static prompt, axes, and stance fit the subject). When parsing or validation fails for a suggestion, we log this reasoning alongside the raw suggestion so we can iteratively improve the meta-prompt and validators.

Net effect:

- Users only see commands they can actually say.
- We gain concrete evidence (per-suggestion reasoning + invalid strings) to refine the `model suggest` prompt and contract over time.

---

## Context

From ADR 008 (prompt recipe suggestion assistant) and ADR 040/042:

- `model suggest` lets the LLM propose 3–5 prompt recipes of the form:
  - `staticPrompt · completeness · scope · method · style · directional`
- The suggestions GUI shows each recipe as:
  - `[Name]`
  - `Say: model …` (contract command)
  - `Axes: C:… S:… M:… St:… D:…`
  - Optional stance/why:
    - `Say: <stance_command>`
    - `Stance: Who=…, Why=…`
    - `Why: …`
- ADR 042 introduces Persona/Intent stance as structured axes:
  - `persona_voice`, `persona_audience`, `persona_tone`, `intent_purpose`, plus
  - a free-form, voice-friendly `stance_command` string.

Current limitations:

- The **contract command** (`Say: model run …`) is effectively validated, because it is derived from a parsed/normalised `recipe` and the same axis/directional maps used by the `model` grammar.
- The **stance command** (`Say: <stance_command>`) is generated by the LLM and only constrained via prompt text; it is not validated in code.
- We have observed invalid stance commands in the wild, such as:
  - `persona as facilitator to stakeholders gently · intent for collaborating` (no such grammar),
  - `model write for` (incomplete axis combo).
- When parsing fails (for example, malformed JSON from the LLM), we currently have no structured explanation from the LLM about *how* it constructed the suggestion, which makes prompt iteration guessy.

We need a way to:

- Guarantee that commands shown in the suggestions GUI are executable under the real Talon grammar.
- Capture enough per-suggestion reasoning from the LLM to debug failed/parsing/validation cases.

---

## Decision

We will:

1. **Treat all LLM-authored command strings as untrusted until validated.**

   - `stance_command` for each suggestion MUST be validated in code before being surfaced as a `Say:` line in the GUI.
   - The contract `Say: model run …` line remains derived solely from the parsed `recipe` and local axis maps, and is already effectively validated.

2. **Define a precise contract for valid `stance_command` forms.**

   The allowed forms are:

   - **Preferred, always valid**:

     - `model write <persona_voice> <persona_audience> <persona_tone> <intent_purpose>`

     where:

     - `<persona_voice>` is either empty or one token from `GPT/lists/modelVoice.talon-list` (for example, `as programmer`, `as teacher`, `as facilitator`, …).
     - `<persona_audience>` is either empty or one token from `GPT/lists/modelAudience.talon-list` (for example, `to stakeholders`, `to team`, …).
     - `<persona_tone>` is either empty or one token from `GPT/lists/modelTone.talon-list` (for example, `gently`, `directly`, …).
     - `<intent_purpose>` is **required** and must be a token from `GPT/lists/modelIntent.talon-list` (for example, `for teaching`, `for collaborating`, …).

     In practice, this means:

     - NEVER allow bare `model write` or `model write for`.
     - When the LLM uses this form, validators must ensure the tail tokens are drawn exclusively from the Persona/Intent axis vocab and include at least one valid intent token.

   - **Optional, preset-based**:

     - `persona <personaPreset>` and/or `intent <intentPreset>`

     where:

     - `<personaPreset>` is a key or spoken label from `PERSONA_PRESETS` in `lib/personaConfig.py` (for example, `persona teach junior dev`, `persona peer engineer explanation`).
     - `<intentPreset>` is a key or spoken label from `INTENT_PRESETS` (for example, `intent teach`, `intent decide`).

     Combined forms such as `persona teach junior dev · intent teach` are allowed when both halves validate.

   - **Prohibited forms** include (non-exhaustive):

     - `persona as facilitator to stakeholders gently` (raw axis tokens after `persona`).
     - `intent for collaborating` (raw intent token after `intent`).
     - Any `stance_command` that mixes axis tokens and preset syntax in ways not described above.

3. **Add a `_valid_stance_command` validator and reject invalid stance commands.**

   - Implement a helper in `GPT/gpt.py` (near the suggestion parser):

     - `def _valid_stance_command(cmd: str) -> bool:`
       - Strips and lowercases the command.
       - Accepts:
         - `model write …` only when all tail tokens are from the union of Persona/Intent axis token sets and at least one valid intent token is present.
         - `persona <preset>` / `intent <preset>` only when names match the current Persona/Intent preset spoken forms.
         - Combined `persona … · intent …` only when each side validates independently.
       - Returns `False` for:
         - Empty strings.
         - Bare `model write`, `model write for`.
         - Unknown tokens or malformed combinations.

   - When parsing LLM suggestions (JSON or legacy lines):

     - Extract `stance_command` as today.
     - Invoke `_valid_stance_command`.
     - Only include `"stance_command"` in the suggestion dict if validation passes.
     - If validation fails:
       - Drop `stance_command` from that suggestion.
       - Optionally record a debug log entry (see reasoning capture below).

   - The suggestions GUI already behaves correctly when `stance_command` is absent: it simply does not render a `Say: …` stance line.

4. **Add per-suggestion reasoning to the LLM contract.**

   - Extend the `model suggest` JSON contract (ADR 042) so each suggestion also includes a `reasoning` field:

     ```json
     {
       "name": "…",
       "recipe": "…",
       "persona_voice": "…",
       "persona_audience": "…",
       "persona_tone": "…",
       "intent_purpose": "…",
       "stance_command": "…",
       "why": "…",
       "reasoning": "short description of how you chose this suggestion"
     }
     ```

   - `reasoning` is **not** shown to the user in the suggestions GUI. It is purely for logging and debugging.

   - Prompt guidance:

     - `reasoning` should summarise, in 1–3 sentences:
       - Why this static prompt and axis combination fit the subject.
       - Why this Persona/Intent stance (if any) was chosen.
       - Any trade-offs or assumptions that influenced the suggestion.

5. **Log invalid suggestions with reasoning for prompt iteration.**

   When parsing or validating suggestions:

   - For each suggestion that fails:

     - JSON/type structure issues (missing name/recipe, malformed JSON).
     - `recipe` normalisation failures (no directional, unknown directional, etc.).
     - `stance_command` validation failures.

   - In **debug mode** (`GPTState.debug_enabled == True`):

     - Emit a debug log entry (for example, via `_debug` or `notify` once per `model suggest` call) including:
       - The suggestion name (if available).
       - The raw `recipe` / `stance_command` that failed.
       - The `reasoning` string for that suggestion, if present.

   - In **normal mode**:

     - Suppress invalid commands quietly.
     - Only fall back to inserting raw output when *no* suggestions were successfully parsed.

6. **Scope validation to LLM-authored commands, not locally constructed ones.**

   - The `Say: model run …` contract lines in the suggestions GUI are derived from:

     - The parsed `recipe` string, using `_normalise_recipe` and axis maps.
     - `suggestion_grammar_phrase`, which already mirrors the `model` grammar.

   - We do **not** add extra string-level validation there; instead, we continue to enforce validity via the existing recipe parsing + axis/directional checks.

---

## Consequences

### Benefits

- **No invalid commands shown to users**

  - Stance commands that are not executable (`model write for`, `persona as facilitator …`, etc.) are never surfaced as `Say:` lines.
  - The GUI remains a trustworthy source of voiceable commands.

- **Improved prompt/debug feedback loop**

  - For each invalid suggestion, we keep both:
    - The bad value (for example, `stance_command="model write for"`), and
    - The LLM’s `reasoning` about why it produced that suggestion.
  - This makes it much easier to refine the meta-prompt and validation rules iteratively, based on evidence.

- **Clearer separation of concerns**

  - Contract commands (`model run …`) remain grounded in local axis parsing.
  - Stance commands get a dedicated validator that knows the Persona/Intent and preset vocab.

### Risks and mitigations

1. **Risk: Silently dropping stance commands could hide useful but slightly off-format suggestions.**

   - Mitigation:
     - In debug mode, always log the invalid `stance_command` and the corresponding `reasoning` so we can adjust the validator or prompt as needed.
     - Keep validators conservative but not overly strict; for example, allow whitespace variation but enforce token sets.

2. **Risk: Validator falls out of sync with grammar or lists.**

   - Mitigation:
     - Build the validator token sets directly from the existing lists and presets:
       - `modelVoice.talon-list`, `modelAudience.talon-list`, `modelTone.talon-list`, `modelIntent.talon-list` for axis tokens.
       - `PERSONA_PRESETS`, `INTENT_PRESETS` for preset names.
     - Add tests that assert `_valid_stance_command` accepts known-good examples (from docs/ADRs) and rejects known-bad ones.

3. **Risk: Logging reasoning leaks too much detail in normal use.**

   - Mitigation:
     - Only emit logs tied to `reasoning` when `GPTState.debug_enabled` is true.
     - Keep user-facing notifications generic; use detailed reasoning only for debug logs.

---

## Implementation Sketch

1. **Prompt changes (`GPT/gpt.py`)**

   - In `gpt_suggest_prompt_recipes_with_source`, extend the JSON contract in `user_text` to include `reasoning` and the refined `stance_command` rules above.

   - Example shape in the prompt:

     ```json
     {
       "suggestions": [
         {
           "name": string,
           "recipe": string,
           "persona_voice": string,
           "persona_audience": string,
           "persona_tone": string,
           "intent_purpose": string,
           "stance_command": string,
           "why": string,
           "reasoning": string
         }
       ]
     }
     ```

2. **Validator helper (`GPT/gpt.py`)**

   - Add `_valid_stance_command(cmd: str) -> bool` and supporting token sets:

     - Build axis token sets once from `personaConfig` and the talon lists:

       - `VOICE_TOKENS`, `AUDIENCE_TOKENS`, `TONE_TOKENS`, `PURPOSE_TOKENS` and `AXIS_TOKEN_SET = union(...)`.
       - `PERSONA_PRESET_SPOKEN_SET`, `INTENT_PRESET_SPOKEN_SET` derived from `PERSONA_PRESETS` / `INTENT_PRESETS` and the spoken forms used for `user.personaPreset` / `user.intentPreset`.

     - Implement logic described in the Decision section.

   - Update the suggestion parsing block:

     - When handling JSON suggestions:

       ```python
       raw_stance = str(item.get("stance_command", "")).strip()
       if _valid_stance_command(raw_stance):
           entry["stance_command"] = raw_stance
       ```

     - When handling legacy lines (pipe-separated format):

       - Same logic when constructing `entry`.

3. **Reasoning capture and debug logging (`GPT/gpt.py`)**

   - During JSON parsing, also read:

     ```python
     reasoning = str(item.get("reasoning", "")).strip()
     if reasoning:
         entry["reasoning"] = reasoning
     ```

   - Maintain a per-suggestion debug record while parsing and validating:

     ```python
     debug = {
         "name": name,
         "raw": {
             "recipe": recipe_value,
             "persona_voice": persona_voice,
             "persona_audience": persona_audience,
             "persona_tone": persona_tone,
             "intent_purpose": intent_purpose,
             "stance_command": raw_stance,
             "reasoning": reasoning,
         },
         "failures": [],  # e.g. ["recipe_invalid_directional", "stance_invalid_form"]
     }
     ```

   - As each validation step runs, append failure codes to `debug["failures"]` when something is rejected:

     - Recipe issues: `"recipe_missing_directional"`, `"recipe_unknown_directional"`, etc.
     - Axis issues: `"persona_intent_invalid_purpose"`, etc.
     - Stance issues: `"stance_invalid_form"`, `"stance_unknown_preset"`, etc.

   - When a suggestion is rejected (missing name/recipe, invalid recipe, invalid stance command):

     - Always append the `debug` record to an in-memory list, e.g.:

       ```python
       debug_failures.append(debug)
       ```

     - After parsing, emit a single low-noise debug log summarising failures (for example with `_debug` or a single `notify` with a truncated view), showing for each failed suggestion:

       - `name`
       - `failures`
       - Selected `raw` fields (for example, `recipe`, `stance_command`, `intent_purpose`)
       - `reasoning` when available

     This log should be concise enough that it is safe to emit on every `model suggest` call, since failures are expected to be rare once the prompt is stable.

4. **Suggestions GUI changes (`lib/modelSuggestionGUI.py`)**

   - The GUI already checks for `suggestion.stance_command` and shows:

     ```python
     if has_stance_command:
         draw_text(f"Say: {stance_text}", x + 4, row_y)
         row_y += line_h
     ```

   - No further change needed: invalid stance commands simply won’t be present in the Suggestion objects.

5. **Tests**

   - Add new tests (for example, `_tests/test_gpt_suggest_validation.py`):

     - Unit tests for `_valid_stance_command`:

       - Accept:
         - `model write as teacher to junior engineer kindly for teaching`
         - `persona teach junior dev`
         - `intent teach`
         - `persona teach junior dev · intent teach`.
       - Reject:
         - `model write`
         - `model write for`
         - `persona as facilitator to stakeholders gently`
         - `intent for collaborating`.

     - Parsing tests:

       - Given a mock `result.text` JSON with mixed-good/bad `stance_command` values and `reasoning`, assert that:
         - Only valid `stance_command`s end up in `record_suggestions` entries.
         - In debug mode, invalid ones are captured in a debug log hook (can be tested via a small injected logger or monkeypatched `notify`).

---

## Adversarial Stress Tests

1. **LLM returns `model write for` as stance_command**

   - Expected:
     - `_valid_stance_command` returns `False`.
     - The suggestion still appears, but without a `Say: model write for` stance line.
     - In debug mode, we log the invalid stance plus its `reasoning`.

2. **LLM returns `persona as facilitator to stakeholders gently` as stance_command**

   - Expected:
     - Validator rejects it (unknown preset name after `persona`).
     - The GUI never shows this line.

3. **LLM returns valid JSON but one suggestion has no directional in `recipe`**

   - Expected:
     - `_normalise_recipe` rejects that recipe; the suggestion is dropped entirely.
     - Other valid suggestions still appear.
     - Debug mode logs which suggestion was dropped and its `reasoning`.

4. **LLM ignores `reasoning` and omits it**

   - Expected:
     - Parsing still succeeds; `reasoning` remains optional.
     - When validation fails, we log only the raw command/recipe without reasoning.

5. **All suggestions fail validation**

   - Expected:
     - No entries passed to `record_suggestions`.
     - We fall back to inserting the raw output into the buffer so the user can see it and adjust manually.
     - In debug mode, we log a summary of all failures and their (possibly missing) `reasoning`.

If these adversarial tests hold, the suggestions GUI will only surface commands that are actually sayable, and we will have enough structured evidence (per-suggestion reasoning + invalid strings) to continuously refine the `model suggest` prompt and validators.
