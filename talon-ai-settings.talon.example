# This is an example settings file.
# To make changes, copy this into your user directory and remove the .example extension

settings():
    # Works with any API with the same schema as OpenAI's (i.e. Azure, llamafiles, etc.)
    # user.model_endpoint = "https://api.openai.com/v1/chat/completions"

    # user.model_system_prompt = "You are an assistant helping an office worker to be more productive."

    # Maximum time in seconds to wait for a single model HTTP request before timing out.
    # user.model_request_timeout_seconds = 120

    # Change to 'gpt-4' or the model of your choice
    # user.openai_model = 'gpt-3.5-turbo'

    # Increase the window width.
    # user.model_window_char_width = 120

    # Enable debug logging for the request progress pill overlay.
    # When set to 1, the pill emits small notifications with its state/position.
    # Default is on; set to 0 to silence pill/UI-thread debug logs.
    user.model_debug_pill = 1

    # Enable streaming responses by default (set to 0 to disable).
    user.model_streaming = 1

    # Optional: directory where `model source save file` writes markdown files
    # If unset, Talon defaults to a `talon-ai-model-sources` folder under the
    # Talon user directory.
    # user.model_source_save_directory = "/path/to/talon-ai-model-sources"

# Only uncomment the line below if you want experimental behavior to parse Talon files
# tag(): user.gpt_beta

# Use codeium instead of Github Copilot
# tag(): user.codeium
