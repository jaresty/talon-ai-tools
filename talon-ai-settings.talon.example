# This is an example settings file.
# To make changes, copy this into your user directory and remove the .example extension

settings():
    # Works with any API with the same schema as OpenAI's (i.e. Azure, llamafiles, etc.)
    # user.model_endpoint = "https://api.openai.com/v1/chat/completions"

    # Optional: switch default provider (ADR 047). Bundled options: openai, gemini.
    # Set tokens via Talon settings or env vars (OPENAI_API_KEY / GEMINI_API_KEY):
    # user.model_provider_token_openai = "sk-..."
    # user.model_provider_token_gemini = "your-gemini-key"
    # Optional provider defaults:
    # user.model_provider_model_gemini = "gemini-1.5-pro"
    # Optional spoken aliases for Gemini models (comma-separated spoken:modelid):
    # user.model_provider_model_aliases_gemini = "one five pro:gemini-1.5-pro,flash:gemini-1.5-flash"
    # Optional spoken aliases for OpenAI models (comma-separated spoken:modelid):
    # user.model_provider_model_aliases_openai = "four o:gpt-4o,four point one:gpt-4.1"
    # user.model_provider_current = "openai"
    # user.model_provider_default = "openai"
    # user.model_provider_extra = {"providers": [
    #     {"id": "local", "display_name": "Local Llama", "endpoint": "http://localhost:8080/v1/chat/completions", "api_key_env": "LOCAL_LLM_API_KEY", "default_model": "llama-3-8b"}
    # ]}
    # Enable reachability probes in provider list/status canvases
    # user.model_provider_probe = 1

    # user.model_system_prompt = "The assistant provides productivity guidance tailored to an office worker's needs."

    # Maximum time in seconds to wait for a single model HTTP request before timing out.
    # user.model_request_timeout_seconds = 120

    # Change to 'gpt-4' or the model of your choice
    # user.openai_model = 'gpt-3.5-turbo'

    # Increase the window width.
    # user.model_window_char_width = 120

    # Enable debug logging for the request progress pill overlay.
    # When set to 1, the pill emits small notifications with its state/position.
    # Default is on; set to 0 to silence pill/UI-thread debug logs.
    user.model_debug_pill = 1

    # Enable streaming responses by default (set to 0 to disable).
    user.model_streaming = 1

    # Optional: directory where the `file` destination (including history saves) writes markdown files
    # If unset, Talon defaults to a `talon-ai-model-sources` folder under the
    # Talon user directory.
    # user.model_source_save_directory = "/path/to/talon-ai-model-sources"

    # Optional: absolute path to the Go binary used for CLI auto-packaging.
    # Set this when Talonâ€™s PATH omits Go (common on macOS sandboxed processes).
    # Example: user.cli_go_command = "/opt/homebrew/bin/go"

# Only uncomment the line below if you want experimental behavior to parse Talon files
# tag(): user.gpt_beta

# Use codeium instead of Github Copilot
# tag(): user.codeium
